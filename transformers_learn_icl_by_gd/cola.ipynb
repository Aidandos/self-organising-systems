{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "#@title Imports external sources\n",
    "import os\n",
    "import io\n",
    "import PIL.Image, PIL.ImageDraw, PIL.ImageFont\n",
    "import base64\n",
    "import zipfile\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pylab as pl\n",
    "import numpy as np\n",
    "import glob\n",
    "import requests\n",
    "import random as pyrandom\n",
    "from concurrent import futures\n",
    "from functools import partial\n",
    "from scipy.ndimage import rotate\n",
    "from IPython.display import Image, HTML, clear_output\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import time\n",
    "from typing import Any, MutableMapping, NamedTuple, Tuple, Optional\n",
    "\n",
    "# !pip install --quiet --upgrade jax\n",
    "# !pip install --quiet --upgrade jaxlib \n",
    "import jax\n",
    "from jax import grad, jit, vmap\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# !pip install --quiet -U dm-haiku\n",
    "# !pip install --quiet -U optax\n",
    "import haiku as hk\n",
    "import optax\n",
    "import math\n",
    "# !pip install --quiet -U ml_collections\n",
    "from ml_collections import config_dict\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.colors as mcolors\n",
    "colors = pl.colormaps['Dark2'] \n",
    "\n",
    "# !pip install ipython-autotime\n",
    "\n",
    "markers = ['o', 'v', '^', '*',  's', 'D']\n",
    "colors = ['blue', 'red', 'green', 'purple']\n",
    "# sns.set_theme(style=\"darkgrid\")\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "fontsize=30\n",
    "\n",
    "game = 'Chicken Game' #@param [\"Matching Pennies\", \"IPD\", \"Ultimatum\", \"Chicken Game\", \"Tandem\", \"Balduzzi\", \"Hamiltonian\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Internal Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer import Transformer\n",
    "from src.data import create_reg_data_classic_token, create_weights\n",
    "from src.config import config\n",
    "from src.train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.96 #@param {type:\"number\"}\n",
    "NUM_RUNS =  10#@param {type:\"number\"}\n",
    "NUM_EPOCHS =  500#@param {type:\"number\"}\n",
    "ALPHA =  5.0#@param {type:\"number\"}\n",
    "BETA =   5.0#@param {type:\"number\"}\n",
    "SMOOTHING = 0.99\n",
    "\n",
    "if game == 'IPD':\n",
    "  INPUT_DIM = 10\n",
    "  STD = 0.1\n",
    "else:\n",
    "  INPUT_DIM = 2\n",
    "  STD =  1.0\n",
    "\n",
    "if game in ['Tandem', 'Balduzzi', 'Hamiltonian']:\n",
    "  BATCH_SIZE =  8\n",
    "  NUM_INNERLOOP_SHORT =  120000\n",
    "  NUM_INNERLOOP_LONG =  120000\n",
    "  NUM_NODES =  8\n",
    "  interval = 1\n",
    "  LR_SCHEDULER = 0.8\n",
    "  LR = 1e-1\n",
    "else:\n",
    "  BATCH_SIZE =  64\n",
    "  NUM_INNERLOOP_SHORT =  80000\n",
    "  NUM_INNERLOOP_LONG =  80000\n",
    "  NUM_NODES =  16\n",
    "  interval = 7\n",
    "  LR_SCHEDULER = 1.0\n",
    "  LR=0.001\n",
    "\n",
    "OUTPUT_DIM=INPUT_DIM//2\n",
    "\n",
    "hyper_params = {\n",
    "    \"gamma\": GAMMA,\n",
    "    \"num_runs\": NUM_RUNS,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"alpha\": ALPHA,\n",
    "    \"std\": STD,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_innerloop_short\": NUM_INNERLOOP_SHORT,\n",
    "    \"num_innerloop_long\": NUM_INNERLOOP_LONG,\n",
    "    \"num_nodes\": NUM_NODES,\n",
    "    \"beta\": BETA,\n",
    "    \"interval\": interval,\n",
    "    \"input_dim\": INPUT_DIM,\n",
    "    \"output_dim\": OUTPUT_DIM,\n",
    "    \"lr_scheduler\": LR_SCHEDULER,\n",
    "    \"lr\": LR,\n",
    "    \"smoothing\": SMOOTHING\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tandem():\n",
    "    dims = [1, 1]\n",
    "\n",
    "    def Ls(th):\n",
    "        x, y = th\n",
    "        # Tandem loss (quadratic loss for moving forward + linear penalty for pedalling backwards)\n",
    "        L_1 = (x + y)**2 - 2.0 * x\n",
    "        L_2 = (x + y)**2 - 2.0 * y\n",
    "        return [L_1, L_2]\n",
    "    return dims, Ls\n",
    "\n",
    "\n",
    "def tandem_cubed():\n",
    "    dims = [1, 1]\n",
    "\n",
    "    def Ls(th):\n",
    "        x, y = th\n",
    "        # Tandem loss (quadratic loss for moving forward + linear penalty for pedalling backwards)\n",
    "        L_1 = (x + y)**4 - 2.0 * x\n",
    "        L_2 = (x + y)**4 - 2.0 * y\n",
    "        return [L_1, L_2]\n",
    "    return dims, Ls\n",
    "\n",
    "\n",
    "def ultimatum():\n",
    "  dims = [1, 1]\n",
    "  def Ls(th):\n",
    "    x, y = th\n",
    "    p_fair = jax.nn.sigmoid(x)\n",
    "    p_accept = jax.nn.sigmoid(y)\n",
    "    L_1 = -(5*p_fair + 8*(1-p_fair)*p_accept)\n",
    "    L_2 = -(5*p_fair + 2*(1-p_fair)*p_accept)\n",
    "    return [L_1, L_2]\n",
    "  return dims, Ls\n",
    "\n",
    "\n",
    "def balduzzi():\n",
    "  dims = [1, 1]\n",
    "  def Ls(th):\n",
    "    x, y = th\n",
    "    L_1 = 0.5*(x**2) + 10*x*y\n",
    "    L_2 = 0.5*(y**2) - 10*x*y\n",
    "    return [L_1, L_2]\n",
    "  return dims, Ls\n",
    "\n",
    "\n",
    "def hamiltonian_game():\n",
    "  dims=[1, 1]\n",
    "  def Ls(th):\n",
    "    x, y = th\n",
    "    L_1 = x*y\n",
    "    L_2 = -x*y\n",
    "    return [L_1, L_2]\n",
    "  return dims, Ls\n",
    "\n",
    "\n",
    "def matching_pennies():\n",
    "  dims = [1, 1]\n",
    "  payout_mat_1 = jnp.array([[1,-1],[-1,1]])\n",
    "  payout_mat_2 = -payout_mat_1\n",
    "  def Ls(th):\n",
    "    p_1, p_2 = jax.nn.sigmoid(th[0]), jax.nn.sigmoid(th[1])\n",
    "    x, y = jnp.concatenate([p_1, 1-p_1]), jnp.concatenate([p_2, 1-p_2])\n",
    "    L_1 = jnp.matmul(jnp.matmul(x, payout_mat_1), y)\n",
    "    L_2 = jnp.matmul(jnp.matmul(x, payout_mat_2), y)\n",
    "    return [L_1, L_2]\n",
    "  return dims, Ls\n",
    "\n",
    "\n",
    "def matching_pennies_batch(batch_size=128):\n",
    "  dims = [1, 1]\n",
    "  payout_mat_1 = jnp.array([[1,-1],[-1,1]])\n",
    "  payout_mat_2 = -payout_mat_1\n",
    "  payout_mat_1 = payout_mat_1.reshape((1, 2, 2)).repeat(batch_size, 1, 1)\n",
    "  payout_mat_2 = payout_mat_2.reshape((1, 2, 2)).repeat(batch_size, 1, 1)\n",
    "  def Ls(th):\n",
    "    p_1, p_2 = jax.nn.sigmoid(th[0]), jax.nn.sigmoid(th[1])\n",
    "    x, y = jnp.concatenate([p_1, 1-p_1], dim=-1), jnp.concatenate([p_2, 1-p_2], dim=-1)\n",
    "    L_1 = jnp.matmul(jnp.matmul(x.unsqueeze(1), payout_mat_1), y.unsqueeze(-1))\n",
    "    L_2 = jnp.matmul(jnp.matmul(x.unsqueeze(1), payout_mat_2), y.unsqueeze(-1))\n",
    "    return [L_1.squeeze(-1), L_2.squeeze(-1)]\n",
    "  return dims, Ls\n",
    "\n",
    "\n",
    "def chicken_game():\n",
    "  dims = [1, 1]\n",
    "  payout_mat_1 = jnp.array([[0, -1],[1, -100]])\n",
    "  payout_mat_2 = jnp.array([[0, 1],[-1, -100]])\n",
    "  def Ls(th):\n",
    "    p_1, p_2 = jax.nn.sigmoid(th[0]), jax.nn.sigmoid(th[1])\n",
    "    x, y = jnp.concatenate([p_1, 1-p_1]), jnp.concatenate([p_2, 1-p_2])\n",
    "    L_1 = -jnp.matmul(jnp.matmul(x, payout_mat_1), y)\n",
    "    L_2 = -jnp.matmul(jnp.matmul(x, payout_mat_2), y)\n",
    "    return [L_1, L_2]\n",
    "  return dims, Ls\n",
    "\n",
    "\n",
    "def chicken_game_batch(batch_size=128):\n",
    "  dims = [1, 1]\n",
    "  payout_mat_1 = jnp.array([[0, -1],[1, -100]])\n",
    "  payout_mat_2 = jnp.array([[0, 1],[-1, -100]])\n",
    "  payout_mat_1 = payout_mat_1.reshape((1, 2, 2)).repeat(batch_size, 1, 1)\n",
    "  payout_mat_2 = payout_mat_2.reshape((1, 2, 2)).repeat(batch_size, 1, 1)\n",
    "  def Ls(th):\n",
    "    p_1, p_2 = jax.nn.sigmoid(th[0]), jax.nn.sigmoid(th[1])\n",
    "    x, y = jnp.concatenate([p_1, 1-p_1], dim=-1), jnp.concatenate([p_2, 1-p_2], dim=-1)\n",
    "    L_1 = -jnp.matmul(jnp.matmul(x.unsqueeze(1), payout_mat_1), y.unsqueeze(-1))\n",
    "    L_2 = -jnp.matmul(jnp.matmul(x.unsqueeze(1), payout_mat_2), y.unsqueeze(-1))\n",
    "    return [L_1.squeeze(-1), L_2.squeeze(-1)]\n",
    "  return dims, Ls\n",
    "  \n",
    "\n",
    "def ipd_batched(gamma=0.96):\n",
    "  dims = [5, 5]\n",
    "  payout_mat_1 = jnp.array([[-1,-3],[0,-2]])\n",
    "  payout_mat_2 = payout_mat_1.T\n",
    "  payout_mat_1 = payout_mat_1.reshape((1, 2, 2)).repeat(hyper_params['batch_size'], 1, 1)\n",
    "  payout_mat_2 = payout_mat_2.reshape((1, 2, 2)).repeat(hyper_params['batch_size'], 1, 1)\n",
    "  def Ls(th):\n",
    "    p_1_0 = jax.nn.sigmoid(th[0][:, 0:1])\n",
    "    p_2_0 = jax.nn.sigmoid(th[1][:, 0:1])\n",
    "    p = jnp.concatenate([p_1_0*p_2_0, p_1_0*(1-p_2_0), (1-p_1_0)*p_2_0, (1-p_1_0)*(1-p_2_0)], dim=-1)\n",
    "    p_1 = jnp.reshape(jax.nn.sigmoid(th[0][:, 1:5]), (hyper_params['batch_size'], 4, 1))\n",
    "    p_2 = jnp.reshape(jax.nn.sigmoid(th[1][:, 1:5]), (hyper_params['batch_size'], 4, 1))\n",
    "    P = jnp.concatenate([p_1*p_2, p_1*(1-p_2), (1-p_1)*p_2, (1-p_1)*(1-p_2)], dim=-1)\n",
    "    x = jnp.eye(4).reshape((1, 4, 4))\n",
    "    eyes = x.repeat(hyper_params['batch_size'], 1, 1)\n",
    "\n",
    "\n",
    "    M = -jnp.matmul(p.unsqueeze(1), jnp.linalg.inv(jnp.eye(4)-gamma*P))\n",
    "    L_1 = jnp.matmul(M, jnp.reshape(payout_mat_1, (hyper_params['batch_size'], 4, 1)))\n",
    "    L_2 = jnp.matmul(M, jnp.reshape(payout_mat_2, (hyper_params['batch_size'], 4, 1)))\n",
    "    return [L_1.squeeze(-1), L_2.squeeze(-1)]\n",
    "  return dims, Ls\n",
    "\n",
    "\n",
    "def ipd(gamma=0.96):\n",
    "  dims = [5, 5]\n",
    "  payout_mat_1 = jnp.array([[-1,-3],[0,-2]])\n",
    "  payout_mat_2 = payout_mat_1.T\n",
    "  def Ls(th):\n",
    "    p_1_0 = jax.nn.sigmoid(th[0][0:1])\n",
    "    p_2_0 = jax.nn.sigmoid(th[1][0:1])\n",
    "    p = jnp.concatenate([p_1_0*p_2_0, p_1_0*(1-p_2_0), (1-p_1_0)*p_2_0, (1-p_1_0)*(1-p_2_0)])\n",
    "    p_1 = jnp.reshape(jax.nn.sigmoid(th[0][1:5]), (4, 1))\n",
    "    p_2 = jnp.reshape(jax.nn.sigmoid(th[1][1:5]), (4, 1))\n",
    "    P = jnp.concatenate([p_1*p_2, p_1*(1-p_2), (1-p_1)*p_2, (1-p_1)*(1-p_2)], axis=1)\n",
    "    M = -jnp.matmul(p, jnp.linalg.inv(jnp.eye(4)-gamma*P))\n",
    "    L_1 = jnp.matmul(M, jnp.reshape(payout_mat_1, (4, 1)))\n",
    "    L_2 = jnp.matmul(M, jnp.reshape(payout_mat_2, (4, 1)))\n",
    "    return [L_1, L_2]\n",
    "  return dims, Ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRelu(hk.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_dim: int,\n",
    "            output_dim: int,\n",
    "            name: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.linear1 = hk.Linear(hidden_dim,\n",
    "                                w_init=hk.initializers.RandomNormal(mean=0, stddev=0.1),\n",
    "                                b_init=hk.initializers.Constant(0),\n",
    "                                )\n",
    "        self.linear2 = hk.Linear(hidden_dim,\n",
    "                                w_init=hk.initializers.RandomNormal(mean=0, stddev=0.1),\n",
    "                                b_init=hk.initializers.Constant(0),\n",
    "                                )\n",
    "        self.linear3 = hk.Linear(output_dim,\n",
    "                                w_init=hk.initializers.RandomNormal(mean=0, stddev=0.1),\n",
    "                                b_init=hk.initializers.Constant(0),\n",
    "                                )\n",
    "        self.relu = jax.nn.ReLU()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "class NetTanh(hk.Module):\n",
    "    def __init__(self,\n",
    "            hidden_dim: int,\n",
    "            output_dim: int,\n",
    "            name: Optional[str] = None,\n",
    "                 ):\n",
    "        super().__init__(name=name)\n",
    "        self.main_body = hk.nets.MLP(\n",
    "            [hidden_dim, hidden_dim, hidden_dim, output_dim],\n",
    "            w_init=hk.initializers.RandomNormal(mean=0, stddev=0.1),\n",
    "            b_init=hk.initializers.Constant(0),\n",
    "            activate_final=False,\n",
    "            activation=jnp.tanh,\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        return self.main_body(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tanh_network():\n",
    "    def forward_fn(x):\n",
    "        return NetTanh(hyper_params['num_nodes'], hyper_params['output_dim'])(x)\n",
    "    network = hk.without_apply_rng(hk.transform(forward_fn))\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1:  1.7698076\n",
      "Loss 2:  1.8566042\n",
      "Loss 1:  1.5100355\n",
      "Loss 2:  1.618503\n",
      "Loss 1:  1.4860876\n",
      "Loss 2:  1.584396\n",
      "Loss 1:  1.4730017\n",
      "Loss 2:  1.5651535\n",
      "Loss 1:  1.4641087\n",
      "Loss 2:  1.5523251\n",
      "Loss 1:  1.4567251\n",
      "Loss 2:  1.5422539\n",
      "Loss 1:  1.4488493\n",
      "Loss 2:  1.5324996\n",
      "Loss 1:  1.4373871\n",
      "Loss 2:  1.5198469\n",
      "Loss 1:  1.4127336\n",
      "Loss 2:  1.4949945\n",
      "Loss 1:  1.3205707\n",
      "Loss 2:  1.4029701\n",
      "Loss 1:  1.1003684\n",
      "Loss 2:  1.1621422\n",
      "Loss 1:  1.0713732\n",
      "Loss 2:  1.1465768\n",
      "Loss 1:  1.0320722\n",
      "Loss 2:  1.1208047\n",
      "Loss 1:  1.0029134\n",
      "Loss 2:  1.1014441\n",
      "Loss 1:  0.9895268\n",
      "Loss 2:  1.0942748\n",
      "Loss 1:  0.982332\n",
      "Loss 2:  1.0921891\n",
      "Loss 1:  0.97750777\n",
      "Loss 2:  1.0922587\n",
      "Loss 1:  0.9737406\n",
      "Loss 2:  1.0935657\n",
      "Loss 1:  0.9704256\n",
      "Loss 2:  1.0958728\n",
      "Loss 1:  0.9671755\n",
      "Loss 2:  1.0993227\n",
      "Loss 1:  0.96358985\n",
      "Loss 2:  1.1044354\n",
      "Loss 1:  0.9590684\n",
      "Loss 2:  1.1124297\n",
      "Loss 1:  0.95236516\n",
      "Loss 2:  1.126219\n",
      "Loss 1:  0.94028753\n",
      "Loss 2:  1.15402\n",
      "Loss 1:  0.9133317\n",
      "Loss 2:  1.2252698\n",
      "Loss 1:  0.9006468\n",
      "Loss 2:  1.4567435\n",
      "Loss 1:  1.4481807\n",
      "Loss 2:  1.525898\n",
      "Loss 1:  1.4399269\n",
      "Loss 2:  1.5181094\n",
      "Loss 1:  1.4212881\n",
      "Loss 2:  1.5008447\n",
      "Loss 1:  1.3588852\n",
      "Loss 2:  1.4418716\n",
      "Loss 1:  1.1742467\n",
      "Loss 2:  1.2537985\n",
      "Loss 1:  1.0755423\n",
      "Loss 2:  1.1770842\n",
      "Loss 1:  0.9953851\n",
      "Loss 2:  1.1134411\n",
      "Loss 1:  0.97987366\n",
      "Loss 2:  1.1032528\n",
      "Loss 1:  0.9731476\n",
      "Loss 2:  1.1008159\n",
      "Loss 1:  0.96879286\n",
      "Loss 2:  1.1011089\n",
      "Loss 1:  0.96520483\n",
      "Loss 2:  1.103221\n",
      "Loss 1:  0.96161586\n",
      "Loss 2:  1.1072873\n",
      "Loss 1:  0.95731425\n",
      "Loss 2:  1.114273\n",
      "Loss 1:  0.95111823\n",
      "Loss 2:  1.1267341\n",
      "Loss 1:  0.9402355\n",
      "Loss 2:  1.1518683\n",
      "Loss 1:  0.9167772\n",
      "Loss 2:  1.2147658\n",
      "Loss 1:  0.8999946\n",
      "Loss 2:  1.4182864\n",
      "Loss 1:  1.4472728\n",
      "Loss 2:  1.5308228\n",
      "Loss 1:  1.4436702\n",
      "Loss 2:  1.5270852\n",
      "Loss 1:  1.4366337\n",
      "Loss 2:  1.5206342\n",
      "Loss 1:  1.4198477\n",
      "Loss 2:  1.50598\n",
      "Loss 1:  1.3592145\n",
      "Loss 2:  1.451681\n",
      "Loss 1:  1.1452732\n",
      "Loss 2:  1.2363584\n",
      "Loss 1:  1.0194467\n",
      "Loss 2:  1.1358175\n",
      "Loss 1:  0.98346305\n",
      "Loss 2:  1.1083238\n",
      "Loss 1:  0.9740338\n",
      "Loss 2:  1.1030608\n",
      "Loss 1:  0.96895874\n",
      "Loss 2:  1.1020508\n",
      "Loss 1:  0.9652413\n",
      "Loss 2:  1.1031467\n",
      "Loss 1:  0.9618469\n",
      "Loss 2:  1.1060625\n",
      "Loss 1:  0.95809126\n",
      "Loss 2:  1.1113284\n",
      "Loss 1:  0.9530923\n",
      "Loss 2:  1.1205584\n",
      "Loss 1:  0.94505584\n",
      "Loss 2:  1.1380572\n",
      "Loss 1:  0.9291424\n",
      "Loss 2:  1.1772009\n",
      "Loss 1:  0.8943923\n",
      "Loss 2:  1.2912031\n",
      "Loss 1:  1.1769329\n",
      "Loss 2:  1.5580091\n",
      "Loss 1:  1.0831543\n",
      "Loss 2:  1.1865077\n",
      "Loss 1:  0.986762\n",
      "Loss 2:  1.104728\n",
      "Loss 1:  0.97640884\n",
      "Loss 2:  1.0963461\n",
      "Loss 1:  0.971922\n",
      "Loss 2:  1.0930288\n",
      "Loss 1:  0.9692882\n",
      "Loss 2:  1.091347\n",
      "Loss 1:  0.96749103\n",
      "Loss 2:  1.0904355\n",
      "Loss 1:  0.9661405\n",
      "Loss 2:  1.0899689\n",
      "Loss 1:  0.96504647\n",
      "Loss 2:  1.0897964\n",
      "Loss 1:  0.9641101\n",
      "Loss 2:  1.0898502\n",
      "Loss 1:  0.9632643\n",
      "Loss 2:  1.090096\n",
      "Loss 1:  0.9624618\n",
      "Loss 2:  1.0905242\n",
      "Loss 1:  0.96166676\n",
      "Loss 2:  1.0911484\n",
      "Loss 1:  0.9608411\n",
      "Loss 2:  1.0919975\n",
      "Loss 1:  0.95994467\n",
      "Loss 2:  1.0931275\n",
      "Loss 1:  0.9589221\n",
      "Loss 2:  1.0946282\n",
      "Loss 1:  0.9576943\n",
      "Loss 2:  1.0966489\n",
      "Loss 1:  0.9561373\n",
      "Loss 2:  1.0994477\n",
      "Loss 1:  0.9540298\n",
      "Loss 2:  1.1034911\n",
      "Loss 1:  0.9509399\n",
      "Loss 2:  1.1096973\n",
      "Loss 1:  0.9459205\n",
      "Loss 2:  1.1201212\n",
      "Loss 1:  0.936496\n",
      "Loss 2:  1.1401614\n",
      "Loss 1:  0.91455626\n",
      "Loss 2:  1.1878116\n",
      "Loss 1:  0.8472764\n",
      "Loss 2:  1.3420686\n",
      "Loss 1:  0.7523417\n",
      "Loss 2:  1.7477434\n",
      "Loss 1:  1.136011\n",
      "Loss 2:  1.325055\n",
      "Loss 1:  1.0055321\n",
      "Loss 2:  1.2602718\n",
      "Loss 1:  1.1864743\n",
      "Loss 2:  1.4330112\n",
      "Loss 1:  1.0015111\n",
      "Loss 2:  1.1310043\n",
      "Loss 1:  0.97613674\n",
      "Loss 2:  1.1114835\n",
      "Loss 1:  0.968509\n",
      "Loss 2:  1.1076716\n",
      "Loss 1:  0.96400374\n",
      "Loss 2:  1.1074796\n",
      "Loss 1:  0.960364\n",
      "Loss 2:  1.1095164\n",
      "Loss 1:  0.9566219\n",
      "Loss 2:  1.1139591\n",
      "Loss 1:  0.95185965\n",
      "Loss 2:  1.1221337\n",
      "Loss 1:  0.9444312\n",
      "Loss 2:  1.137736\n",
      "Loss 1:  0.9300852\n",
      "Loss 2:  1.1720372\n",
      "Loss 1:  0.8974126\n",
      "Loss 2:  1.2686969\n",
      "Loss 1:  1.0340977\n",
      "Loss 2:  1.5487162\n",
      "Loss 1:  1.3417478\n",
      "Loss 2:  1.4210699\n"
     ]
    }
   ],
   "source": [
    "_, Ls = ipd()\n",
    "# def Ls(th):\n",
    "#     agent1, agent2 = th\n",
    "#     L_1 = agent1*agent2\n",
    "#     L_2 = -(agent1*agent2)\n",
    "#     return [L_1, L_2]\n",
    "\n",
    "def loss1_fn(theta1, theta2):\n",
    "    L1, _ = Ls([theta1, theta2])\n",
    "    # print(L1.shape)\n",
    "    return L1[0]\n",
    "\n",
    "def loss2_fn(theta1, theta2):\n",
    "    _, L2 = Ls([theta1, theta2])\n",
    "    # print(L2.shape)\n",
    "    return L2[0]\n",
    "\n",
    "def lola_loss1_fn(theta1, theta2):\n",
    "    theta2_prime = theta2 - lr2 * grad(loss2_fn, argnums=1)(theta1, theta2)\n",
    "    L1, _ = Ls([theta1, theta2_prime])\n",
    "    # print(L1.shape)\n",
    "    return L1[0]\n",
    "\n",
    "def lola_loss2_fn(theta1, theta2):\n",
    "    theta1_prime = theta1 - lr1 * grad(loss1_fn, argnums=0)(theta1, theta2)\n",
    "    _, L2 = Ls([theta1_prime, theta2])\n",
    "    # print(L1.shape)\n",
    "    return L2[0]\n",
    "\n",
    "def update(theta1, theta2, lr1=0.1, lr2=0.1):\n",
    "    return theta1 - lr1 * grad(loss1_fn, argnums=0)(theta1, theta2), theta2 - lr2 * grad(loss2_fn, argnums=1)(theta1, theta2)\n",
    "\n",
    "\n",
    "def update_lola(theta1, theta2, lr1=0.1, lr2=0.1):\n",
    "    # theta1_prime = theta1 - lr1 * grad(loss1_fn, argnums=0)(theta1, theta2)\n",
    "    # theta2_prime = theta2 - lr2 * grad(loss2_fn, argnums=1)(theta1, theta2)\n",
    "    return theta1 - lr1 * grad(lola_loss1_fn, argnums=0)(theta1, theta2), theta2 - lr2 * grad(lola_loss2_fn, argnums=1)(theta1, theta2)\n",
    "\n",
    "rng = jax.random.PRNGKey(400)\n",
    "# network = make_tanh_network()\n",
    "# theta1 = network.init(rng, jnp.zeros((hyper_params['input_dim'],)))\n",
    "# theta2 = network.init(rng, jnp.zeros((hyper_params['input_dim'],)))\n",
    "theta1 = jax.random.normal(rng, (5,))\n",
    "rng, rng2 = jax.random.split(rng)\n",
    "theta2 = jax.random.normal(rng2, (5,))\n",
    "lr1 = lr2 = 1.0\n",
    "for i in range(100):\n",
    "    theta1, theta2 = update_lola(theta1, theta2, lr1=lr1, lr2=lr2)\n",
    "    print(\"Loss 1: \", (1-hyper_params['gamma'])*loss1_fn(theta1, theta2))\n",
    "    print(\"Loss 2: \", (1-hyper_params['gamma'])*loss2_fn(theta1, theta2))\n",
    "# grad(loss1_fn, argnums=0)(theta1, theta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Playing Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import Optional\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from src.attn import (MLP,\n",
    "                      MultiHeadAttention,\n",
    "                      TokenVocab,\n",
    "                      create_pos_encoding,\n",
    "                      LNorm,\n",
    "                      layer_norm)\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Transformer(hk.Module):\n",
    "  \"\"\"A flexible Transformer implementation.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      num_heads: int = 2,\n",
    "      widening_factor: int = 4,\n",
    "      num_layers: int = 3,\n",
    "      key_size: int = 5,\n",
    "      embedding_size: int = 64,\n",
    "      output_size: int = 1,\n",
    "      in_context_length: int = 17,\n",
    "      in_context_length_test: int = 17,\n",
    "      test_points: int = 1,\n",
    "      dropout_rate: float = 0,\n",
    "      only_attention: bool = True,\n",
    "      use_layer_norm: bool = True,\n",
    "      use_pe: bool = True,\n",
    "      pe_size: int = 6,\n",
    "      concat_pe: bool = False,\n",
    "      output_mapping: bool = False,\n",
    "      input_mapping: bool = False,\n",
    "      use_bias_p: bool = True,\n",
    "      zero_embeddings: bool = False,\n",
    "      deq: bool = True,\n",
    "      init_scale: float = 0.02,\n",
    "      use_softmax: bool = False,\n",
    "      use_non_lin_mix: bool = False,\n",
    "      first_layer_sm: bool = False,\n",
    "      y_update: bool = False,\n",
    "      input_mlp: bool = False,\n",
    "      input_mlp_out_dim: int = 0,\n",
    "      gd_mlp_config: bool = False,\n",
    "      sum_norm: bool = False,\n",
    "      dampening: float = 1.0,\n",
    "      clip: float = 0.0,\n",
    "      ana_copy: bool = False,\n",
    "      flip: bool = False,\n",
    "      vocab_size: int = 0,\n",
    "      vocab_token_dim: int = 0,\n",
    "      vocab_init: int = 0.01,\n",
    "      return_logits: bool = False,\n",
    "      include_query: bool = False,\n",
    "      name: Optional[str] = None,\n",
    "  ):\n",
    "\n",
    "\n",
    "    \"\"\"Initialises the module.\n",
    "\n",
    "    Args:\n",
    "      num_heads: Number of heads in the self-attention module.\n",
    "      widening_factor: Blow up in the hidden layer of MLP.\n",
    "      num_layers: Number of transformer layers, usually one due DEQ behaviour.\n",
    "      key_size: Key and querie size.\n",
    "      embedding_size: Embedding size.\n",
    "      output_size: Output size.\n",
    "      in_context_length: Sequence length.\n",
    "      test_points: Number of test points.\n",
    "      dropout_rate: Optional dropout layer with rate dropout_rate if not None.\n",
    "      only_attention: Only the attention layer without the MLP.\n",
    "      use_layer_norm: Use layer norm or not.\n",
    "      use_pe: Use positional encoding. \n",
    "      pe_size: Positional encoding size.\n",
    "      concat_pe: Concat pe.\n",
    "      output_mapping: Use output mapping.\n",
    "      input_mapping: Use input mapping.\n",
    "      lin_proj_after_att: Linear projection after attention layer.\n",
    "      use_bias_p: Use bias parameter in the linear operations in the network.\n",
    "      zero_embeddings: Use zero embeddings.\n",
    "      full_state_update: Update context tokens or only querry.\n",
    "      deq: Use recurrent transformer.\n",
    "      y_update: Update only output states e.g. as in gradient descent.\n",
    "      input_mlp: Use MLP instead of linear embedding.\n",
    "      input_mlp_out_dim: Output dim of input MLP.\n",
    "      gd_mlp_config: Gradient descent special MLP config.\n",
    "      sum_norm: Use sum normalization from Schlag et. al 2012\n",
    "      dampening: Dampen forward dynamics\n",
    "      clip: Clip the activations to some value\n",
    "      ana_copy: Return full prediction stack instead of last entry.\n",
    "      include_query: Include query vector in computation.\n",
    "      name : Optional name for this module.\n",
    "    \"\"\"\n",
    "\n",
    "    super().__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.widening_factor = widening_factor\n",
    "    self.num_layers = num_layers\n",
    "    self.key_size = key_size\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.only_attention = only_attention\n",
    "    self.use_layer_norm = use_layer_norm\n",
    "    self.use_pe = use_pe\n",
    "    self.pe_size = pe_size\n",
    "    self.concat_pe = concat_pe\n",
    "    self.output_mapping = output_mapping\n",
    "    self.input_mapping = input_mapping\n",
    "    self.use_bias_p = use_bias_p\n",
    "    self.embedding_size = embedding_size\n",
    "    self.output_size = output_size\n",
    "    self.in_context_length = in_context_length\n",
    "    self.in_context_length_test = in_context_length_test\n",
    "    self.zero_embeddings = zero_embeddings\n",
    "    self.init_scale = init_scale\n",
    "    self.use_softmax = use_softmax\n",
    "    self.use_non_lin_mix = use_non_lin_mix\n",
    "    self.first_layer_sm = first_layer_sm\n",
    "    self.deq = deq\n",
    "    self.y_update = y_update\n",
    "    self.input_mlp = input_mlp\n",
    "    self.input_mlp_out_dim = input_mlp_out_dim\n",
    "    self.gd_mlp_config = gd_mlp_config\n",
    "    self.sum_norm = sum_norm\n",
    "    self.dampening = dampening\n",
    "    self.clip = clip\n",
    "    self.ana_copy = ana_copy\n",
    "    self.vocab_size = vocab_size\n",
    "    self.vocab_token_dim = vocab_token_dim\n",
    "    self.vocab_init = vocab_init\n",
    "    self.return_logits = return_logits\n",
    "    self.include_query = include_query\n",
    "\n",
    "    if pe_size > 0:\n",
    "      self.pos_encoding = create_pos_encoding(in_context_length, pe_size, flip)\n",
    "      self.pos_encoding_test = create_pos_encoding(in_context_length_test,\n",
    "                                                   pe_size, flip)\n",
    "    else:\n",
    "      self.pos_encoding = None\n",
    "\n",
    "  def trans_block(self, h, nl):\n",
    "    # First the attention block.\n",
    "\n",
    "    if self.deq:\n",
    "      h_norm = self.lnorm1(h) if self.use_layer_norm else h\n",
    "      if not self.include_query:\n",
    "        key = h_norm[:, :-1, :]\n",
    "        value = h_norm[:, :-1, :]\n",
    "      else:\n",
    "        key = h_norm\n",
    "        value = h_norm\n",
    "\n",
    "      h_attn, att_map =self.attn_block(h_norm,key,value)\n",
    "\n",
    "    h_attn = hk.dropout(hk.next_rng_key(), self.dropout_rate, h_attn)\n",
    "\n",
    "    h = h + self.dampening*h_attn\n",
    "\n",
    "    if self.clip > 0:\n",
    "      h = jnp.clip(h, -self.clip, self.clip)\n",
    "\n",
    "    return h, att_map\n",
    "\n",
    "  def __call__(\n",
    "      self,\n",
    "      x: jnp.ndarray,\n",
    "      is_training: bool,\n",
    "      predict_test: bool\n",
    "  ) -> jnp.ndarray:\n",
    "\n",
    "    \"\"\"Computes the transformer forward pass.\n",
    "\n",
    "    Args:\n",
    "      x: Inputs.\n",
    "      is_training: Whether we're training or not.\n",
    "      predict_test: Test or train prediction.\n",
    "    Returns:\n",
    "      Array of shape [B, T, H].\n",
    "    \"\"\"\n",
    "\n",
    "    self.w_init = hk.initializers.VarianceScaling(self.init_scale)\n",
    "    self.dropout_rate = self.dropout_rate if is_training else 0.\n",
    "\n",
    "    embeddings = x\n",
    "\n",
    "    h = embeddings\n",
    "\n",
    "    if len(h.shape) == 2:\n",
    "      _, model_size = h.shape\n",
    "    elif len(h.shape) == 3:\n",
    "      _, _, model_size = h.shape\n",
    "    self.model_size = model_size\n",
    "    if self.deq:\n",
    "      self.attn_block = MultiHeadAttention(num_heads=self.num_heads,\n",
    "                                           key_size=self.key_size,\n",
    "                                           model_size=model_size,\n",
    "                                           w_init=self.w_init,\n",
    "                                           use_softmax=self.use_softmax,\n",
    "                                           use_non_lin_mix=self.use_non_lin_mix,\n",
    "                                           use_bias_p=self.use_bias_p,\n",
    "                                           sum_normalization=self.sum_norm\n",
    "                                           )\n",
    "\n",
    "    st = h[:, -1, -1]*(-1.0) if not self.ana_copy else (h if self.include_query else h[:, :-1, :])\n",
    "    stack_h = [] if not self.input_mlp else [st]\n",
    "    stack_att = []\n",
    "    for nl in range(self.num_layers):\n",
    "      h, att_map = self.trans_block(h, nl)\n",
    "      # intermediate readout of test prediction\n",
    "      st = h[:, -1, -1]*(-1.0) if not self.ana_copy else (h if self.include_query else h[:, :-1, :])\n",
    "      stack_h.append(st)\n",
    "      stack_att.append(att_map)\n",
    "    out = hk.Linear(self.output_size)(h) if self.output_mapping else h\n",
    "\n",
    "    return(out, stack_h, stack_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_Metrics = MutableMapping[str, Any]\n",
    "\n",
    "class TrainState(NamedTuple):\n",
    "  \"\"\"Container for the training state.\"\"\"\n",
    "  params: hk.Params\n",
    "  opt_state: optax.OptState\n",
    "  rng: jnp.DeviceArray\n",
    "  step: jnp.DeviceArray\n",
    "\n",
    "def forward(tokens: jnp.ndarray, is_training: bool, gd: bool):\n",
    "    \"\"\"Transformer forward.\"\"\"\n",
    "    if config.classic_token_const:\n",
    "        in_context_length = config.dataset_size*2 + 1\n",
    "    else:\n",
    "        in_context_length = config.dataset_size + 1\n",
    "        \n",
    "    tr = Transformer(\n",
    "        num_heads=config.num_heads,\n",
    "        num_layers=config.num_layers,\n",
    "        widening_factor=config.widening_factor,\n",
    "        key_size=config.key_size,\n",
    "        embedding_size=config.emb_size,\n",
    "        only_attention=config.att_only_trans,\n",
    "        in_context_length=in_context_length,\n",
    "        output_size=config.output_size,\n",
    "        dropout_rate=config.dropout_rate,\n",
    "        use_pe=config.pos_enc,\n",
    "        pe_size=config.pos_enc_size,\n",
    "        concat_pe=config.concat_pos_enc,\n",
    "        output_mapping=config.out_proj,\n",
    "        input_mapping=config.in_proj,\n",
    "        use_layer_norm=config.layer_norm,\n",
    "        use_bias_p=config.use_bias,\n",
    "        deq=config.deq,\n",
    "        y_update=config.y_update,\n",
    "        use_softmax=config.use_softmax,\n",
    "        use_non_lin_mix=config.use_non_lin_mix,\n",
    "        first_layer_sm=config.first_layer_sm,\n",
    "        zero_embeddings=config.zero_pos_enc,\n",
    "        init_scale=config.init_scale,\n",
    "        input_mlp=config.input_mlp,\n",
    "        input_mlp_out_dim=config.input_mlp_out_dim,\n",
    "        sum_norm=config.sum_norm,\n",
    "        dampening=config.dampening,\n",
    "        clip=config.clip,\n",
    "        ana_copy=config.ana_copy\n",
    "        )\n",
    "\n",
    "\n",
    "    return tr(tokens, is_training=is_training, predict_test=False)\n",
    "\n",
    "\n",
    "def compute_loss(preds, targets):\n",
    "    assert preds.shape == targets.shape\n",
    "    return 0.5*jnp.sum((targets-preds)**2)/targets.shape[0]\n",
    "\n",
    "@hk.transform\n",
    "def loss_fn(data: jnp.ndarray, gd) -> jnp.ndarray:\n",
    "    \"\"\"Computes the MSE loss between targets and predictions.\"\"\"\n",
    "    preds, _, _ = forward(data[0], True, gd)\n",
    "    targets = data[1][:, -1]\n",
    "    preds = preds[:, -1, -1]*(-1.0)\n",
    "    return compute_loss(preds, targets)\n",
    "\n",
    "_, Ls = ipd()\n",
    "# def Ls(th):\n",
    "#     agent1, agent2 = th\n",
    "#     L_1 = agent1*agent2\n",
    "#     L_2 = -(agent1*agent2)\n",
    "#     return [L_1, L_2]\n",
    "\n",
    "def loss1_fn(theta1, theta2):\n",
    "    theta1 = forward\n",
    "    L1, _ = Ls([theta1, theta2])\n",
    "    # print(L1.shape)\n",
    "    return L1[0]\n",
    "\n",
    "def loss2_fn(theta1, theta2):\n",
    "    _, L2 = Ls([theta1, theta2])\n",
    "    # print(L2.shape)\n",
    "    return L2[0]\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2))\n",
    "def update(state: TrainState, optimiser, gd=False)->Tuple[TrainState, _Metrics]:\n",
    "    \"\"\"Does an SGD step and returns training state as well as metrics.\"\"\"\n",
    "    rng, new_rng = jax.random.split(state.rng)\n",
    "    jit_loss_apply = jit(loss_fn.apply, static_argnums=3)\n",
    "    loss_and_grad_fn = jax.value_and_grad(jit_loss_apply)\n",
    "    loss, gradients = loss_and_grad_fn(state.params, rng, gd)\n",
    "\n",
    "    updates, new_opt_state = optimiser.update(gradients, state.opt_state,\n",
    "                                                state.params)\n",
    "    new_params = optax.apply_updates(state.params, updates)\n",
    "\n",
    "    new_state = TrainState(\n",
    "        params=new_params,\n",
    "        opt_state=new_opt_state,\n",
    "        rng=new_rng,\n",
    "        step=state.step + 1,\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        'step': state.step,\n",
    "        'train_loss': loss,\n",
    "    }\n",
    "    return new_state, metrics\n",
    "\n",
    "\n",
    "def init_model(rng, optimiser) -> TrainState:\n",
    "    \"\"\"Init haiku transform modules to create train and test state.\"\"\"\n",
    "    train_rng, test_rng = jax.random.split(rng, num=2)\n",
    "\n",
    "    initial_params = loss_fn.init(rng, gd=False)\n",
    "    initial_opt_state = optimiser.init(initial_params)\n",
    "\n",
    "    return TrainState(\n",
    "        params=initial_params,\n",
    "        opt_state=initial_opt_state,\n",
    "        rng=train_rng,\n",
    "        step=np.array(0))\n",
    "\n",
    "\n",
    "def init():\n",
    "    \"\"\"Init data creator, model, optimizer, etc.\"\"\"\n",
    "    rng = jax.random.PRNGKey(config.seed)\n",
    "    rng, train_rng = jax.random.split(rng, 2)\n",
    "\n",
    "    lr = config.lr\n",
    "    if config.adam:\n",
    "        optimiser = optax.chain(\n",
    "            optax.clip_by_global_norm(config.grad_clip_value),\n",
    "            optax.adamw(learning_rate=lr, b1=config.b1, b2=config.b2,\n",
    "                        weight_decay=config.wd),\n",
    "        )\n",
    "    else:\n",
    "        optimiser = optax.chain(\n",
    "            optax.clip_by_global_norm(config.grad_clip_value),\n",
    "            optax.sgd(learning_rate=lr,),\n",
    "        )\n",
    "\n",
    "    train_state, test_state = init_model(rng, optimiser)\n",
    "    return optimiser, train_state, test_state, rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loss_fn() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m optimiser1, train_state1, _, rng \u001b[39m=\u001b[39m init()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m optimiser2, train_state2, _, rng \u001b[39m=\u001b[39m init()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n",
      "\u001b[1;32m/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=135'>136</a>\u001b[0m     optimiser \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m         optax\u001b[39m.\u001b[39mclip_by_global_norm(config\u001b[39m.\u001b[39mgrad_clip_value),\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m         optax\u001b[39m.\u001b[39msgd(learning_rate\u001b[39m=\u001b[39mlr,),\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m     )\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=140'>141</a>\u001b[0m train_state, test_state \u001b[39m=\u001b[39m init_model(rng, optimiser)\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39mreturn\u001b[39;00m optimiser, train_state, test_state, rng\n",
      "\u001b[1;32m/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Init haiku transform modules to create train and test state.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m train_rng, test_rng \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msplit(rng, num\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m initial_params \u001b[39m=\u001b[39m loss_fn\u001b[39m.\u001b[39;49minit(rng, gd\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m initial_opt_state \u001b[39m=\u001b[39m optimiser\u001b[39m.\u001b[39minit(initial_params)\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mreturn\u001b[39;00m TrainState(\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m     params\u001b[39m=\u001b[39minitial_params,\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m     opt_state\u001b[39m=\u001b[39minitial_opt_state,\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m     rng\u001b[39m=\u001b[39mtrain_rng,\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f74696d6f6e5f677075222c2273657474696e6773223a7b22686f7374223a227373683a2f2f666c6169722d6e6f64652d30312e726f626f74732e6f782e61632e756b227d7d/home/duser/self-organising-systems/transformers_learn_icl_by_gd/cola.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m     step\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray(\u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/haiku/_src/transform.py:165\u001b[0m, in \u001b[0;36mwithout_state.<locals>.init_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_fn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m   params, state \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49minit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m   \u001b[39mif\u001b[39;00m state:\n\u001b[1;32m    167\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf your transformed function uses `hk.\u001b[39m\u001b[39m{\u001b[39m\u001b[39mget,set}_state` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mthen use `hk.transform_with_state`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.9/site-packages/haiku/_src/transform.py:416\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.init_fn\u001b[0;34m(rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mwith\u001b[39;00m base\u001b[39m.\u001b[39mnew_context(rng\u001b[39m=\u001b[39mrng) \u001b[39mas\u001b[39;00m ctx:\n\u001b[1;32m    415\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    417\u001b[0m   \u001b[39mexcept\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     \u001b[39mraise\u001b[39;00m jax\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: loss_fn() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "optimiser1, train_state1, _, rng = init()\n",
    "optimiser2, train_state2, _, rng = init()\n",
    "for i in range(100):\n",
    "    train_state1, metrics = update(train_state1, optimiser1)\n",
    "    train_state2, metrics = update(train_state1, optimiser2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICS",
   "language": "python",
   "name": "ics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
